---
  title: "Skills Quiz: Residuals, Sums of Squares, and R-squared"
output: 
  html_document:
  theme: cerulean
code_folding: hide
toc: true
toc_float: true
---
  
  
  ## Instructions
  
  Use this file to keep a record of your work as you complete the "Skills Quiz: Residuals, Sums of Squares, and R-squared" assignment in Canvas.

```{r}
library(ggplot2)
```

----
  
  <!-- Note: The {} after each Problem and Part header allows you to keep track of what work you have completed. Write something like {Done} once you complete each problem and your html file will then show you a nice summary of what you have "done" already. -->
  
  ## Problem 1 {}
  
  Open the `Orange` dataset in R. As stated in the help file for this data set, "The Orange data... records of the growth of orange trees." 

```{r}
View(Orange)
?Orange
```

Perform a simple linear regression of the circumference of the tree based on its age in days.

```{r}
orange.lm <- lm(circumference ~ age, data=Orange)
summary(orange.lm)
```


### Part (a) {}

Type out the mathematical equation for this regression model and label both $Y$ and $X$ in the equation.

<div class="YourAnswer">
  
  $$
  \underbrace{Y_i}_\text{circumference} = \beta_0 + \beta_1\underbrace{X_i}_\text{age}+\epsilon_i
$$
  
  </div>
  
  
  ### Part (b) {}
  
  Plot a scatterplot of the data with your regression line overlaid.

<div class="YourAnswer">
  
```{r}
ggplot(Orange, aes(x = age, y = circumference))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)
```

</div>
  
  
  ### Part (c) {}
  
  State and interpret the slope and y-intercept of this regression. Are they meaningful for this data?
  
  <div class="YourAnswer">
  
  The slope is 0.11 and the y-intercept is 17.40. Neither are statistically meaningful for this data, but the interpration would be that the slope represents the change in the average circumference as trees age by one day and that the intercept is the average circumference of trees when the trees are first planted (the latter is all together useless as trees don't exist when they are 0 days old. Their just seeds at that point).

</div> 
  
  
  ### Part (d) {}
  
  What are the values of SSE, SSR, SSTO, $R^2$, and the correlation $r$ for this regression?
  
  <div class="YourAnswer">
  
```{r}
SSE <- sum((Orange$circumference - orange.lm$fitted.values)^2)
SSR <- sum((orange.lm$fitted.values - mean(Orange$circumference))^2)
SSTO <- sum((Orange$circumference - mean(Orange$circumference))^2)

correlation <- cor(Orange$circumference, Orange$age)
```

</div>
  
  
  ### Part (e) {}
  
  How do the values of SSR and SSTO relate to the correlation?
  
  <div class="YourAnswer">
  
  The SSR divided by the SSTO gives the proportion of variation in Y explained by the regression, which is R-squared. The square root of R-squared is the correlation.
  
  The square root of SSR divided by SSTO gives the value of the correlation.

</div>
  
  
  ### Part (f) {}
  
  What circumference would we expect orange trees to have on average after 3 years, based on this regression? 
  
  <div class="YourAnswer">
  
```{r}
predict(orange.lm, data.frame(age = 365*3))
```


</div>
  
  
  
  ----
  
  ## Problem 2 {}
  
  Open the **mtcars** dataset in R. Fit three different regression models to the data that can each be used to explain average gas mileage of a vehicle (`mpg`). 

* The first regression should use the weight (`wt`) of the vehicle as the explanatory variable.
```{r}
mpg_wt.lm <- lm(mpg ~ wt, data=mtcars)
```

* The second should use the number of cylinders (`cyl`) of the engine of the vehicle as the explanatory variable.
```{r}
mpg_cyl.lm <- lm(mpg ~ cyl, data=mtcars)
```

* The third should use the gross horsepower of the vehicle (`hp`) as the explanatory variable.
```{r}
mpg_hp.lm <- lm(mpg ~ hp, data=mtcars)
```
 

### Part (a) {}

Plot all three regressions in three separate plots.

<div class="YourAnswer">
  
```{r}
ggplot(mtcars, aes(x = wt, y = mpg))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
ggplot(mtcars, aes(x = cyl, y = mpg))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
ggplot(mtcars, aes(x = hp, y = mpg))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)
```

</div>
  
  
  ### Part (b) {}
  
  State the values of SSE, SSR, SSTO, and $R^2$ for each regression. 


<div class="YourAnswer">
  

```{r}
SSE_wt <- sum((mtcars$mpg - mpg_wt.lm$fitted.values)^2)
SSR_wt <- sum((mpg_wt.lm$fitted.values - mean(mtcars$mpg))^2)
SSTO_wt <- sum((mtcars$mpg - mean(mtcars$mpg))^2)

SSE_cyl <- sum((mtcars$mpg - mpg_cyl.lm$fitted.values)^2)
SSR_cyl <- sum((mpg_cyl.lm$fitted.values - mean(mtcars$mpg))^2)
SSTO_cyl <- sum((mtcars$mpg - mean(mtcars$mpg))^2)

SSE_hp <- sum((mtcars$mpg - mpg_hp.lm$fitted.values)^2)
SSR_hp <- sum((mpg_hp.lm$fitted.values - mean(mtcars$mpg))^2)
SSTO_hp <- sum((mtcars$mpg - mean(mtcars$mpg))^2)

Rsquar_wt <- SSR_wt/SSTO_wt
Rsquar_cyl <- SSR_cyl/SSTO_cyl
Rsquar_hp <- SSR_hp/SSTO_hp

```

</div>
  
  
  ### Part (c) {}
  
  Compare the values from **Part (b)** across each regression. Consider these questions as you compare these numbers.

* What insight do these numbers give about the regression?
  * Which numbers change, which stay the same, in these regressions? Why?
  * Which regression is best at explaining average `mpg` according to these numbers?
  
  <div class="YourAnswer">
  
  Type your answer here...

</div> 
  
  
  
  
  
  ----
  
  Before we can really trust the $R^2$ value from a regression model, there are important diagnostic checks to perform on the regression. 

You now have a better understanding of what a **residual** and a **fitted-value** are in regression. With that improved knowledge, work through the following problem.



----
  
  ## Problem 3 {}
  
  Check the following technical details for each of the three regressions of **Problem 2**.

### Part (a) {}

Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot for each regression of **Problem 2**.

<div class="YourAnswer">
  
```{r}
par(mfrow=c(1,3))
plot(mpg_cyl.lm, which=1:2)
plot(mpg_cyl.lm$residuals)


par(mfrow=c(1,3))
plot(mpg_hp.lm, which=1:2)
plot(mpg_hp.lm$residuals)


par(mfrow=c(1,3))
plot(mpg_wt.lm, which=1:2)
plot(mpg_wt.lm$residuals)

```

</div>
  
  
  ### Part (b) {}
  
  Explain, as best you understand currently, what each of these three plots show for these regressions.

<div class="YourAnswer">
  
  The Residuals vs. Fitted plot checks for the linear relationship and constant variance of the data. If the data seems to curve it's not linear an if the data seems to megaphone the variance is no constant. Bascially, the points should look spread out and random
  
  The Q-Q Plot checks the normality of the error terms. It plots the residuals to see if the sampled residuals match what they were expected to be. If this is so, then both the residuals and error terms are normal. If not, then both the residuals and error terms are not normal.
  
  The Residuals vs. Order Plot checks if the error terms are independent. There should't be any trends or patterns. Their placement should bascially look random.

</div>
  
  
  ### Part (c) {}
  
  Comment on whether or not we should trust the $R^2$ value from each of your three regressions based on your plots in Part (a). 

<div class="YourAnswer">
  
  Type your answer here...

</div>
  
  
  
  ----
  
  
  
  
  
  
  
  
  
  
  
<style>
  
  .YourAnswer {
    color: #317eac;
      padding: 10px;
    border-style: solid;
    border-width: 2px;
    border-color: skyblue4;
    border-radius: 5px;
  }

</style>
  
  
  