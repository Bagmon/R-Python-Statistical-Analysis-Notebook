---
title: "Week 8 | Multiple Linear Regression"
author: "Brian Grant"
date: "6/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Prep, include=FALSE}
library(tidyverse)
library(mosaic)
library(pander)
```
# Day 1 Announcements
Next week we'll start the business consulting project.

# Multiple Linear Regression - Two Lines Model

## Why

## What 
### Math Model
  *Baseline*
  $$
  Baseline = y = \beta_0 + \beta_1x_1
  $$
  
  
## How
### Class Example
use the example in the linear regression page as well.
#### mtcars

So if you had data like what we see below, you'd see two very sepearate groupings. 
```{r}
plot(mpg ~ qsec, 
     data = mtcars, 
     col= am+2)
#this plots mpg ~ qsec with the two layers of mtcars$am colored differently
```

We need to put lines through each of these groups. You do that like follows

```{r}
lmout <- lm(mpg ~ qsec + am + qsec:am,
            data = mtcars)
#this should remind you of Two-factor ANOVA!
summary(lmout)

coef <- lmout$coefficients
coef
#we'll need this to make lines on our plots
```
The coefficicients given in coef are the y-intercept, the 
```{r}
plot(mpg ~ qsec,
     data = mtcars,
     col = am+2)
abline(a=coef[1], 
       b=coef[2], 
       col="red")
abline(a=coef[3] + coef[1], 
       b=coef[4] + coef[2], 
       col="green")
```
The red one is the baseline (first/bottom line). The green one is the change line (second/top line).

Ok, we have the lines, but how do we know if this helped at all?


```{r}
summary(lmout)
```
Well it looks like mtcars$am wasn't statistically significant. Lets remove it.
```{r}
lmout2 <- lm(mpg ~ qsec + qsec:am,
             data=mtcars)
summary(lmout2)
```
first estimate = $\beta_0$
second estimate = $\beta_1$
third estimate = $\beta_2$
fourth estimate = $\beta_3$
```{r}
coef2 <- lmout2$coefficients
coef2
```
Let's plot the new model

```{r}
plot(mpg ~ qsec,
     data = mtcars,
     col = am+2,
     xlim = c(0,30),
     ylim = c(-25,35))
abline(a=coef2[1], 
       b=coef2[2], 
       col="red")
abline(a=coef2[1], 
       b=coef2[3] + coef2[2], 
       col="green")
```

Now the intercept has become statistically significant. 

*Note*
if you're not sure which you should drop, start with the second line and the third and so on in that order. You want to see if there is a need for a second variable at all.


Now lets calculate the estimated values of the slope and intercept of each of the drawn lines.

```{r line 1 Automatic}
#intercept
coef[1]
#slope
coef[2]
#run this in the command line to get the intercept and slope respectively


```

```{r line 2 Manual}
#intercept
coef[1] + coef[3]
#slope
coef[2] + coef[4] 
#run this in the command line to get the intercept and slope respectively
``` 


# Day 2

## Multi Linear Regression Refresher

### in class excercise 1 - equal slopes model


```{r}
mtcars.lm <- lm(mpg ~ qsec + am + qsec:am, data = mtcars)

summary(mtcars.lm)
b <- mtcars.lm$coefficients

palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

abline(b[1], b[2], col=palette()[1])

abline(b[3], b[2], col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")
```

### in class excercise 2 - equal intercepts model (but different slopes)
```{r}
palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

mtcars.lm <- lm(mpg ~ qsec + am + qsec:am, data = mtcars)

summary(mtcars.lm)
b <- mtcars.lm$coefficients

abline( b[1], b[2], col=palette()[1])
abline( b[1], b[2]+b[4], col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")


```

###  class excercise 3 - Full Model (different slopes & different intercepts)
```{r}
palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

mtcars.lm <- lm(mpg ~ qsec + am + qsec:am, data = mtcars)

summary(mtcars.lm)
b <- mtcars.lm$coefficients

abline( b[1], b[2], col=palette()[1])
abline( b[1]+b[3], b[2]+b[4], col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")


```


# Skills Quiz 

## Q1
This problem will have you practice using the "two-lines" multiple regression model from the Math 325 Statistics Notebook.

Run the following commands in R.

```{r}
library(mosaic)
#?SaratogaHouses
```

### 1)
As explained in the help file (?SaratogaHouses), the SaratogaHouses data set contains data about many houses from Saratoga County, New York in the year 2006. When it comes to buying and selling a home, one of the most important factors is determining the value (or price) of the home. Suppose a family is in search of a home that was newly constructed and that has three bedrooms. Suppose further that they are trying to decide how big of a livingArea they can afford and whether or not the price of the home is significantly impacted by adding a fireplace to their "dream home" wish list.

To get a group of homes that are similar to their current specifications run the following codes in R.

```{r}
SH2 <- filter(SaratogaHouses, bedrooms == 3,  newConstruction=="Yes")

#View(SH2)
```


Use the SH2 data set and a "two-lines" multiple regression model to describe the price of a house according to the size of the livingArea of the house and whether or not the house has a fireplace (fireplaces is only 0 or 1 for this reduced SH2 data).


The two-lines regression model for this situation would be most appropriately labeled as:
$$
  \underbrace{Y_i}_\text{price} = \beta_0 + \beta_1 \underbrace{X_{1i}}_\text{livingArea} + \beta_2 \underbrace{X_{2i}}_\text{fireplaces} + \cdots + \beta_{3} X_{1i} X_{2i} + \epsilon_i 
$$
$\beta_0$ = The average price of a home with no fireplace and a living area of zero square feet. Since this is unrealistic, this parameter doesn't actually carry any meaning for this particular regression model.
$\beta_1$ = The change in the average price of a home without a fireplace as the living area increases by 1 additional square foot.
$\beta_2$ = The difference in the average price of a home with a fireplace as compared to a home without a fireplace for homes with zero square feet of living area. 
$\beta_3$ = The change in the effect of 1 additional square foot in the living area on the average price of homes with a fireplace as compared to homes without a fireplace.

### 2)

Perform the above regression in R. To demonstrate that you can do this, fill in the blanks in the following R code statement.

```{r}
sh2.lm <- lm(price ~ livingArea + fireplaces + livingArea:fireplaces,
             data = SH2)

summary(sh2.lm)
sh2.coef <- sh2.lm$coefficients
```

### 3)
There are four places in the R output of the above regression results that contain p-values, one p-value for each coefficient. Note that each of these p-values have a "t value" next to them implying that they came from a t test, which is cool. If you have done your work correctly, the p-value for the test of the hypothesis that livingArea effects the average price of a home is 0.0055.

What is the p-value for the test of the hypotheses that

$H_0 : \beta_3 = 0$
$H_0 : \beta_3 \neq 0$

Answer: p-value = 0.0654 

### 4)
The p-value for the test that β3=0  is not significant at the 0.05 level. However, neither is the p-value for the test of β2=0 . This suggests an interesting idea that either one or both of these variables is not significant. However, mutliple linear regression is a complicated world. It is best practice to "remove" only the "least significant" term from the model and then re-check all p-values to see what is now significant. This is because everything can change dramatically when just one variable is added or removed from the regression. Watch what happens to the summary output when you remove the fireplaces term from the lm(...) but keep the other terms, including the fireplaces:livingArea interaction term, in the model.

```{r}
sh2.lmreduce <- lm(price ~ livingArea + livingArea:fireplaces,
             data = SH2)

summary(sh2.lmreduce)
sh2.reduce.coef <- sh2.lmreduce$coefficients
```

New p-value for the interaction term:   0.0482

This is now significant at the α=0.05 level.

### 5)

It is important to visualize a regression whenever possible so that the reader can connect with the "truth" about the situation that we are trying to show them. Reproduce the two graphics that are shown below. 

 

Only check these boxes if you really made the graphs:

```{r Full Regression Model}
plot(price ~ livingArea,
     data = SH2,
     col = c("skyblue","orange")[as.factor(fireplaces)],
     pch = 21,
     bg = "gray83", 
     main = "Two-lines Model using SH2 data set", 
     cex.main = 1)

legend("topleft",
       legend = c("Baseline (fireplaces == 0)",
                  "Changed-line (fireplaces == 1)"),
       bty = "n",
       lty = 1,
       col = c("skyblue","orange"),
       cex = 0.8)

#get the "Estimates" automatically:
b <- sh2.coef

# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: intercept
# b[2] is the estimate of beta_1:  livingArea
# b[3] is the estimate of beta_2: fireplaces
# b[4] is the estimate of beta_3: livingArea:fireplaces
curve(b[1] + b[2]*x,
      col="skyblue",
      lwd=2,
      add=TRUE)  #baseline (in blue)
curve((b[1] + b[3]) + (b[2] + b[4])*x,
      col="orange", 
      lwd=2,
      add=TRUE) #changed line (in orange)
```

```{r Regression Model with Fireplaces term removed}
plot(price ~ livingArea,
     data = SH2,
     col = c("skyblue","orange")[as.factor(fireplaces)],
     pch = 21,
     bg = "gray83", 
     main = "Two-lines Model using SH2 data set", 
     cex.main = 1)

legend("topleft",
       legend = c("Baseline (fireplaces == 0)",
                  "Changed-line (fireplaces == 1)"),
       bty = "n",
       lty = 1,
       col = c("skyblue","orange"),
       cex = 0.8)

#get the "Estimates" automatically:
b <- sh2.reduce.coef
# Then b will have 3 estimates:
# b[1] is the estimate of beta_0: intercept
# b[2] is the estimate of beta_1:  livingArea
###
# b[3] is the estimate of beta_3: livingArea:fireplaces
curve(b[1] + b[2]*x,
      col="skyblue",
      lwd=2,
      add=TRUE)  #baseline (in blue)
curve((b[1]) + (b[2] + b[3])*x,
      col="orange", 
      lwd=2,
      add=TRUE) #changed line (in orange)
```
### 6)

Now that we have performed the regression, found a "significant model" and drawn the regression, we are ready to interpret the results.


Suppose the family we were discussing earlier found a house that had a livingArea of 2500 square feet and a fireplace. What is the predicted cost (based on your reduced regression model) for this house?

 
```{r}
predict(sh2.lmreduce, 
        data.frame(livingArea = 2500, 
                   fireplaces = 1),
        data = SH2,
        interval = "prediction")
```

Predicted price = $ \$311,915 $ (Round to the nearest whole dollar.)

 

Further, this "reduced model" claims that the average price of a homes without fireplaces increases by $ \$143.13  $
per each additional square foot while the average price for homes with fireplaces increases by $\$16.48 + \$143.13 = 159.61 $ per each additional square foot. (Round both answers to the nearest cent.)

 

Before we ever fully trust the results and interpretation of a regression model, it is important to diagnose the appropriateness of the model. To do this, run the commands in order to create your three diagnostic plots of regression:


```{r}
library(car)

par(mfrow=c(1,3))

plot(sh2.lmreduce, which=1)

qqPlot(sh2.lmreduce$residuals, id=FALSE)

plot(sh2.lmreduce$residuals)
```
 

Demonstrate that you have made these plots by selecting the three rows in the data set that are identified as possible "outliers"

Answer: 2, 4, and 18 
Note that they are represented in the Residuals vs Fitted Plot by the dots that have been numbered.


None of these diagnostic plots look "great" but they are "good enough" to use the regression results for interpretation and prediction.

## Q2

## Q3





