---
title: "Week 8 | Multiple Linear Regression"
author: "Brian Grant"
date: "6/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Prep, include=FALSE}
library(tidyverse)
library(mosaic)
library(pander)
```
# Day 1 Announcements
Next week we'll start the business consulting project.

# Multiple Linear Regression - Two Lines Model

## Why

## What 
### Math Model
  *Baseline*
  $$
  Baseline = y = \beta_0 + \beta_1x_1
  $$
  
  
## How
### Class Example
use the example in the linear regression page as well.
#### mtcars

So if you had data like what we see below, you'd see two very sepearate groupings. 
```{r}
plot(mpg ~ qsec, 
     data = mtcars, 
     col= am+2)
#this plots mpg ~ qsec with the two layers of mtcars$am colored differently
```

We need to put lines through each of these groups. You do that like follows

```{r}
lmout <- lm(mpg ~ qsec + am + qsec:am,
            data = mtcars)
#this should remind you of Two-factor ANOVA!
summary(lmout)

coef <- lmout$coefficients
coef
#we'll need this to make lines on our plots
```
The coefficicients given in coef are the y-intercept, the 
```{r}
plot(mpg ~ qsec,
     data = mtcars,
     col = am+2)
abline(a=coef[1], 
       b=coef[2], 
       col="red")
abline(a=coef[3] + coef[1], 
       b=coef[4] + coef[2], 
       col="green")
```
The red one is the baseline (first/bottom line). The green one is the change line (second/top line).

Ok, we have the lines, but how do we know if this helped at all?


```{r}
summary(lmout)
```
Well it looks like mtcars$am wasn't statistically significant. Lets remove it.
```{r}
lmout2 <- lm(mpg ~ qsec + qsec:am,
             data=mtcars)
summary(lmout2)
```
first estimate = $\beta_0$
second estimate = $\beta_1$
third estimate = $\beta_2$
fourth estimate = $\beta_3$
```{r}
coef2 <- lmout2$coefficients
coef2
```
Let's plot the new model

```{r}
plot(mpg ~ qsec,
     data = mtcars,
     col = am+2,
     xlim = c(0,30),
     ylim = c(-25,35))
abline(a=coef2[1], 
       b=coef2[2], 
       col="red")
abline(a=coef2[1], 
       b=coef2[3] + coef2[2], 
       col="green")
```

Now the intercept has become statistically significant. 

*Note*
if you're not sure which you should drop, start with the second line and the third and so on in that order. You want to see if there is a need for a second variable at all.


Now lets calculate the estimated values of the slope and intercept of each of the drawn lines.

```{r line 1 Automatic}
#intercept
coef[1]
#slope
coef[2]
#run this in the command line to get the intercept and slope respectively


```

```{r line 2 Manual}
#intercept
coef[1] + coef[3]
#slope
coef[2] + coef[4] 
#run this in the command line to get the intercept and slope respectively
``` 


# Day 2

## Multi Linear Regression Refresher

### in class excercise 1 - equal slopes model


```{r}
mtcars.lm <- lm(mpg ~ qsec + am + qsec:am, data = mtcars)

summary(mtcars.lm)
b <- mtcars.lm$coefficients

palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

abline(b[1], b[2], col=palette()[1])

abline(b[3], b[2], col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")
```

### in class excercise 2 - equal intercepts model (but different slopes)
```{r}
palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

mtcars.lm <- lm(mpg ~ qsec + am + qsec:am, data = mtcars)

summary(mtcars.lm)
b <- mtcars.lm$coefficients

abline( b[1], b[2], col=palette()[1])
abline( b[1], b[2]+b[4], col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")


```

###  class excercise 3 - Full Model (different slopes & different intercepts)
```{r}
palette(c("skyblue","firebrick"))

plot(mpg ~ qsec, data=mtcars, col=as.factor(am), xlim=c(0,30), ylim=c(-30,40), main="1974 Motor Trend Cars", pch=16)

mtcars.lm <- lm(mpg ~ qsec + am + qsec:am, data = mtcars)

summary(mtcars.lm)
b <- mtcars.lm$coefficients

abline( b[1], b[2], col=palette()[1])
abline( b[1]+b[3], b[2]+b[4], col=palette()[2])

legend("topleft", legend=c("automatic","manual"), pch=1, col=palette(), title="Transmission (am)", bty="n")


```


# Skills Quiz 

## Q1
This problem will have you practice using the "two-lines" multiple regression model from the Math 325 Statistics Notebook.

Run the following commands in R.

```{r}
library(mosaic)
#?SaratogaHouses
```

### 1)
As explained in the help file (?SaratogaHouses), the SaratogaHouses data set contains data about many houses from Saratoga County, New York in the year 2006. When it comes to buying and selling a home, one of the most important factors is determining the value (or price) of the home. Suppose a family is in search of a home that was newly constructed and that has three bedrooms. Suppose further that they are trying to decide how big of a livingArea they can afford and whether or not the price of the home is significantly impacted by adding a fireplace to their "dream home" wish list.

To get a group of homes that are similar to their current specifications run the following codes in R.

```{r}
SH2 <- filter(SaratogaHouses, bedrooms == 3,  newConstruction=="Yes")

#View(SH2)
```


Use the SH2 data set and a "two-lines" multiple regression model to describe the price of a house according to the size of the livingArea of the house and whether or not the house has a fireplace (fireplaces is only 0 or 1 for this reduced SH2 data).


The two-lines regression model for this situation would be most appropriately labeled as:
$$
  \underbrace{Y_i}_\text{price} = \beta_0 + \beta_1 \underbrace{X_{1i}}_\text{livingArea} + \beta_2 \underbrace{X_{2i}}_\text{fireplaces} + \cdots + \beta_{3} X_{1i} X_{2i} + \epsilon_i 
$$
$\beta_0$ = The average price of a home with no fireplace and a living area of zero square feet. Since this is unrealistic, this parameter doesn't actually carry any meaning for this particular regression model.
$\beta_1$ = The change in the average price of a home without a fireplace as the living area increases by 1 additional square foot.
$\beta_2$ = The difference in the average price of a home with a fireplace as compared to a home without a fireplace for homes with zero square feet of living area. 
$\beta_3$ = The change in the effect of 1 additional square foot in the living area on the average price of homes with a fireplace as compared to homes without a fireplace.

### 2)

Perform the above regression in R. To demonstrate that you can do this, fill in the blanks in the following R code statement.

```{r}
sh2.lm <- lm(price ~ livingArea + fireplaces + livingArea:fireplaces,
             data = SH2)

summary(sh2.lm)
sh2.coef <- sh2.lm$coefficients
```

### 3)
There are four places in the R output of the above regression results that contain p-values, one p-value for each coefficient. Note that each of these p-values have a "t value" next to them implying that they came from a t test, which is cool. If you have done your work correctly, the p-value for the test of the hypothesis that livingArea effects the average price of a home is 0.0055.

What is the p-value for the test of the hypotheses that

$H_0 : \beta_3 = 0$
$H_0 : \beta_3 \neq 0$

Answer: p-value = 0.0654 

### 4)
The p-value for the test that β3=0  is not significant at the 0.05 level. However, neither is the p-value for the test of β2=0 . This suggests an interesting idea that either one or both of these variables is not significant. However, mutliple linear regression is a complicated world. It is best practice to "remove" only the "least significant" term from the model and then re-check all p-values to see what is now significant. This is because everything can change dramatically when just one variable is added or removed from the regression. Watch what happens to the summary output when you remove the fireplaces term from the lm(...) but keep the other terms, including the fireplaces:livingArea interaction term, in the model.

```{r}
sh2.lmreduce <- lm(price ~ livingArea + livingArea:fireplaces,
             data = SH2)

summary(sh2.lmreduce)
sh2.reduce.coef <- sh2.lmreduce$coefficients
```

New p-value for the interaction term:   0.0482

This is now significant at the α=0.05 level.

### 5)

It is important to visualize a regression whenever possible so that the reader can connect with the "truth" about the situation that we are trying to show them. Reproduce the two graphics that are shown below. 

 

Only check these boxes if you really made the graphs:

```{r Full Regression Model}
plot(price ~ livingArea,
     data = SH2,
     col = c("skyblue","orange")[as.factor(fireplaces)],
     pch = 21,
     bg = "gray83", 
     main = "Two-lines Model using SH2 data set", 
     cex.main = 1)

legend("topleft",
       legend = c("Baseline (fireplaces == 0)",
                  "Changed-line (fireplaces == 1)"),
       bty = "n",
       lty = 1,
       col = c("skyblue","orange"),
       cex = 0.8)

#get the "Estimates" automatically:
b <- sh2.coef

# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: intercept
# b[2] is the estimate of beta_1:  livingArea
# b[3] is the estimate of beta_2: fireplaces
# b[4] is the estimate of beta_3: livingArea:fireplaces
curve(b[1] + b[2]*x,
      col="skyblue",
      lwd=2,
      add=TRUE)  #baseline (in blue)
curve((b[1] + b[3]) + (b[2] + b[4])*x,
      col="orange", 
      lwd=2,
      add=TRUE) #changed line (in orange)
```

```{r Regression Model with Fireplaces term removed}
plot(price ~ livingArea,
     data = SH2,
     col = c("skyblue","orange")[as.factor(fireplaces)],
     pch = 21,
     bg = "gray83", 
     main = "Two-lines Model using SH2 data set", 
     cex.main = 1)

legend("topleft",
       legend = c("Baseline (fireplaces == 0)",
                  "Changed-line (fireplaces == 1)"),
       bty = "n",
       lty = 1,
       col = c("skyblue","orange"),
       cex = 0.8)

#get the "Estimates" automatically:
b <- sh2.reduce.coef
# Then b will have 3 estimates:
# b[1] is the estimate of beta_0: intercept
# b[2] is the estimate of beta_1:  livingArea
###
# b[3] is the estimate of beta_3: livingArea:fireplaces
curve(b[1] + b[2]*x,
      col="skyblue",
      lwd=2,
      add=TRUE)  #baseline (in blue)
curve((b[1]) + (b[2] + b[3])*x,
      col="orange", 
      lwd=2,
      add=TRUE) #changed line (in orange)
```
### 6)

Now that we have performed the regression, found a "significant model" and drawn the regression, we are ready to interpret the results.


Suppose the family we were discussing earlier found a house that had a livingArea of 2500 square feet and a fireplace. What is the predicted cost (based on your reduced regression model) for this house?

 
```{r}
predict(sh2.lmreduce, 
        data.frame(livingArea = 2500, 
                   fireplaces = 1),
        data = SH2,
        interval = "prediction")
```

Predicted price = $ \$311,915 $ (Round to the nearest whole dollar.)

 

Further, this "reduced model" claims that the average price of a homes without fireplaces increases by $ \$143.13  $
per each additional square foot while the average price for homes with fireplaces increases by $\$16.48 + \$143.13 = 159.61 $ per each additional square foot. (Round both answers to the nearest cent.)

 

Before we ever fully trust the results and interpretation of a regression model, it is important to diagnose the appropriateness of the model. To do this, run the commands in order to create your three diagnostic plots of regression:


```{r}
library(car)

par(mfrow=c(1,3))

plot(sh2.lmreduce, which=1)

qqPlot(sh2.lmreduce$residuals, id=FALSE)

plot(sh2.lmreduce$residuals)
```
 

Demonstrate that you have made these plots by selecting the three rows in the data set that are identified as possible "outliers"

Answer: 2, 4, and 18 
Note that they are represented in the Residuals vs Fitted Plot by the dots that have been numbered.


None of these diagnostic plots look "great" but they are "good enough" to use the regression results for interpretation and prediction.

## Q2
While the "two-lines" model is a very good way to begin your journey into multiple linear regression, there is a vast number of other possible models that we could use, infintely many actually. This problem will allow you to practice what is called a "high dimensional model."

### 1)
A researcher is interested in knowing what controllable factors influence the rate of vehicle accidents on highways. Data was collected in 1973 on highways in Minnesota in an effort to answer this question.

```{r}

library(car)
# ?Highway1
# View(Highway1)

```

 

Use the Highway1 dataset in R to determine if the speed limit (slim), width of the shoulder on the roadway (shld), and the percentage of vehicle volume that are trucks (trks) have any significant effect on the rate of accidents. 

The mathematical model for this regression is given by

Yi=β0+β1Xi1+β2Xi2+β3Xi3+ϵi

where Yi is the $\text{Rate of accidents: rate}$, Xi1 is the $\text{speed limit: slim}$, Xi2 is the $\text{width of the shoulder: shld}$, and Xi3 is the $\text{percentage of trucks: trks}$.

 
Use R to find an estimated regression equation of the model defined above.

```{r High Dimensional Model}
Highway1.lm <- lm(rate ~
                    slim +
                    shld +
                    trks,
                  data = Highway1)
# NOTE: THERE WAS NO INTERACTION TERM! WHY?! BECAUSE WE'RE COMPARING EACH VARIABLE TO SEE IT'S AFFECT ON THE INTERCEPT I THING???

summary(Highway1.lm)
```

$\hat{Y_i}$=$\text{}$ + $\text{}$Xi1 + 0.02015Xi2 + −0.28175Xi3.

### 2)


Select the variables that have a significant effect (non-zero βi) on the average rate of accidents when all variables stated above are included in the model.

Answer: speed limit & percentage of trucks

### 3)
Check the appropriateness of this regression model by creating the Residuals vs. fitted-values and normal Q-Q plots. One point is labeled as the most extreme outlier in these plots? What number is next to this point?
```{r}
par(mfrom = c(1,3))
plot(Highway1.lm, which = 1:2)
plot(Highway1.lm$residuals)
```


Point # $27$

### 4)
The linear relation assumption for this model is perhaps somewhat questionable, but constant variance and normality of the errors don't pose any real concerns. Continuing with the regression despite the possible difficulty, use your math skills or R skills to predict the rate of accidents if the speed limit = 55, width of shoulder = 6, and Percentage of trucks = 10.

```{r}
predict(Highway1.lm, data.frame(slim = 55,
                                shld = 6,
                                trks = 10))
```

The predicted rate of accidents is  $3.727934$



## Q3
Selecting an appropriate model in multiple regression is a very difficult process. This question will give you some practice at doing this by starting with a simple linear regression model, and trying to add to it "useful" explanatory variables. These are called "added variables" and 

 

A researcher develops a multiple regression model to predict the highway miles per gallon of a vehicle based on the city miles per gallon of the vehicle. They run the following codes to do this.

```{r}
library(mosaic)
# ?mpg
# View(mpg)

plot(hwy ~ cty, data = mpg)
mpg.lm <- lm(hwy ~ cty, data=mpg)

summary(mpg.lm)

par(mfrow=c(1,2)); plot(mpg.lm, which=1:2)

```


Note that the city miles per gallon is a significant predictor of the highway miles per gallon, p-value < 2e-16.

 

The researcher is wondering if their model would be improved by including a second explanatory variable. They are considering whether they should add (A) the number of cylinders of the vehicle, (B) the the drive type of the vehicle, or (C) the displacement of the vehicle to the model. 

Create added variable plots for each of these three variables using the following code

```{r}

par(mfrow=c(1,3))

plot(mpg.lm$residuals ~ cyl , data=mpg)

plot(mpg.lm$residuals ~ as.factor(drv), data=mpg)

plot(mpg.lm$residuals ~ displ, data=mpg)

# note that they just made a plot for each of the variables' residuals 
```


### 1)
Select all variables that appear to show pattterns in the residuals. These patterns imply that there is extra information to add to the mpg.lm regression model. In other words, we should add these variable(s) to the current regression.

Answer: drv only. Though cyl looks like it's a pattern that's jsut because there are very few factors and so everything has been grouped within each one. 

### 2)
To see how a t Test helps to decide this same result, perform three two-variable regressions. Note that in each regression the original explanatory variable of city is included with one of the three extra variables under consideration included each time. For example the first regression would use

```{r}

mpg.lm <- lm(hwy ~ cty + cyl, data=mpg)

summary(mpg.lm)

```

Your turn, now run three such regresions in R where you change out cyl on two of the regressions for drv and then displ, respectively.

```{r}

mpg.lm2 <- lm(hwy ~ cty + drv, data=mpg)

summary(mpg.lm2)

```

```{r}

mpg.lm3 <- lm(hwy ~ cty + displ, data=mpg)

summary(mpg.lm3)

```


What is the p-value for the number of cylinders variable when added to the original mpg.lm regression?

p-value =   $0.466$

This shows that there is  $insufficient$ evidence to conclude that the coefficient for cylinders is different from zero. In other words, cylinders doesn't add anything to the original regression model. 

Note: this is just as we observed above in Q3.2

### 3)
What is the t Test p-value for the displacement of the engine when added to the original mpg.lm regression?

p-value = $0.817 $

This shows that there is $insufficient$ evidence to conclude that the coefficient for displacement of the engine is different from zero. In other words, displacement of the engine doesn't add anything to the original regression model.

Note: again, this is just as we observed above in Q3.2

### 4)
What is the t Test p-value for the drive type of the vehicle when added to the original mpg.lm regression?

Note that since drive type is a categorical variable with 4-wheel, front-wheel, and rear-wheel drive categories, there are two p-values. 

p-value for front-wheel drive types = 1.42e-14
  (copy and paste the answer)

p-value for rear-wheel drive types = 9.70e-10
 (copy and paste the answer)

This shows that there is  $sufficient$ evidence to conclude that the coefficients for front-wheel and rear-wheel drive types is different from zero. In other words, the various drive types add information to the original regression model. They should be included in the model. 

### ?)
Optional: To verify this result, see if you can use mPlot(mpg) to visually recreate a near depiction this regression. Email your code to your instructor to see how you did. Doing this will help you on your Analysis for this week and is recommended even though it is optional.

```{r}
mpg.lm2 <- lm(hwy ~ cty + drv, 
              data=mpg)
mpglm2.coef <- mpg.lm2$coefficients
summary(mpg.lm2)

plot(hwy ~ cty,
     data = mpg,
     col = c("red", "green")[as.factor(drv)]
)

# curve(
#   mpglm2.coef[1] + mpglm2.coef[2]*x, 
#   add = TRUE)
# curve(
#   (mpglm2.coef[1] + mpglm2.coef[3]) + 
#   (mpglm2.coef[2]+ mpglm2.coef[4]*x), 
#   add = TRUE)

curve(
  mpglm2.coef[1] + mpglm2.coef[2]*x,
  add = TRUE,
  col = "red")
# curve(
#   (mpglm2.coef[1] + mpglm2.coef[3]) + 
#   (mpglm2.coef[2]*x), 
#   col = "red",
  # add = TRUE)
curve(
  (mpglm2.coef[1] + mpglm2.coef[4]) + 
  (mpglm2.coef[2]*x), 
  col = "green",
  add = TRUE)
```





