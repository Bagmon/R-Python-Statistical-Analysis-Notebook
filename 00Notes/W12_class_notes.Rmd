---
title: "Week 12 | Chi-Squared"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Prep, include=FALSE}
library(tidyverse)
library(mosaic)
library(pander)
library(car)
```

# Announcements

# Day 1

## Chi Squared Analysis Example


```{r}
#View(Titanic)
#?Titanic
```

Lets see if there was a relationship between passenger survival and ticket class.
```{r}
table(TitanicSurvival$passengerClass, TitanicSurvival$survived)
```


Lets see if there was a relationship between passenger survival and gender.

```{r}
table(TitanicSurvival$sex, TitanicSurvival$survived)
```

### how c() can be used in Chi-squared Analysis

Bascially, you bind c() lists together into a matrix.

Look up rbind & cbind. They turn c() lists into a matrix. rbind turns each list and turns it into a row. cbind does the same but turns them into columns.

```{r}
fruit <- rbind(Apples=c(Good = 80, Bruised = 20, Rotten = 15),
               Oranges=c(Good = 75, Bruised = 25, Rotten = 10)
               )
fruit

fruit <- cbind(Apples=c(Good = 80, Bruised = 20, Rotten = 15),
               Oranges=c(Good = 75, Bruised = 25, Rotten = 10)
               )
fruit
```


If you need to flip the matrix, just use the t() function.
```{r}
t(fruit)
```


### how to graphically summarise the data

Use a barplot.

```{r}
barplot(fruit, beside = TRUE, legend.text = TRUE)
```

Is type of fruit connected to it's condition?
```{r}
chisq.test(fruit)
```
We have sufficient evidence to reject the null and accept that fruit type and condition are related.

# Day 2

## In class activity

Load the Data/GSS2012.csv file.
NOTE: link to the original data is at http://sda.berkeley.edu/sdaweb/analysis/?dataset=gss12 and is from the 2012 Topical Surveys

```{r}
library(readr)
GSS2012 <- read_delim("../Data/GSS2012.csv", 
                      "\t", escape_double = FALSE, trim_ws = TRUE)
names(GSS2012) # This shows the names of the columns
```

Pick any two columns you are interested in (as long as it isn't the "id" column). You'll probably have no idea what the columns represent for now, that's okay. Just pick two of the names.

Say you picked the columns: nameOfColumn1 and nameOfColumn2 then run the codes:

> table(GSS2012$nameOfColumn1)

> table(GSS2012$nameOfColumn2)

> table(GSS2012$nameOfColumn1, GSS2012$nameOfColumn2)

This gives you some insight about how many of each response there are in the data. 

```{r}
library(mosaic)

matrix.a <- GSS2012 %>% select(RATETONE, JOKESWK) %>% filter(RATETONE != 0 & JOKESWK !=0)

table(matrix.a$RATETONE)
table(matrix.a$JOKESWK)

table(matrix.a$RATETONE, matrix.a$JOKESWK)
table(matrix.a$JOKESWK, matrix.a$RATETONE)

```

Turn the three table(...) commands that you just ran into barplots in R. (Open the "Graphical Summaries" page of the Statistics-Notebook if you need help remembering how to do this.)

Show your barplots to your peers, a TA, or the teacher to ensure that you have made them each correctly.

```{r}
barplot(table(matrix.a$RATETONE))

barplot(table(matrix.a$JOKESWK))

barplot(table(matrix.a$RATETONE, matrix.a$JOKESWK))
barplot(table(matrix.a$JOKESWK, matrix.a$RATETONE))

```

Run a chi-squared test on your two columns to see if they are independent or associated with each other.

```{r}
toneXjokes <- table(matrix.a$JOKESWK, matrix.a$RATETONE)

chiresults.toneXjokes <- chisq.test(toneXjokes)
chiresults.toneXjokes
```

Check the requirements of your chi-squared test. Are the results of the test valid?

```{r}
chiresults.toneXjokes$expected
```
Suppose you called your chi-squared test, mychisq in R. Then run

mychisq$residuals

Then, open your Statistics-Notebook to the "Explanation" tab of the "Chi Squared Tests" page. Scroll down until you find the part on "Interpretation" and discuss with your group how "Pearson Residuals" are used to interpret the results of a chi-squared test.

If your group is confused about the Pearson Residuals, ask your teacher for help.
```{r}
chiresults.toneXjokes$residuals
```
Nicely done! You can use this for your Chi-squared analysis this week if you like.

# Skills Quiz

## Q1

Review the Notebook

## Q2
Refer to the Explanation file of your Chi Squared Tests page of your Statistics Notebook for the following questions.

### 1)
 
In the following table, the factor for the rows would be called $Class$ and the factor for the columns would be called $Survival$.

	No	Yes
1st	122	203
2nd	167	118
3rd	528	178
Crew	673	212

### 2)
$H_0$ : The row variable and column variable are independent.
$H_a$ : The row variable and column variable are associated.

The two things needed to obtain a p-value for the chi-squared test of independence are 1) the test statistic χ2=∑mi=1(Oi−Ei)2Ei and 2) the $ distribution$ of the test statistic that is calculated under the assumption that the null hypothesis is true. 

The Ei in the χ2 test statistic formula are called the $expected$ counts and are obtained by multiplying the row total with the $column$ $total$ and dividing by the  $total$ $total$. These values show us what values we would expect to observe if the null hypothesis was true. In other words, they provide the counts we would expect if the row variable and column variable were $independent$. 

 

The χ2 test statistic can be assumed to follow a  $\text{chi-squared distribution}$ with degrees of freedom p=($r - 1$)*(c−1).
  
The chi-squared distribution is a $parametric$ distribution because is has a single parameter known as the degrees of freedom, p.
 

Pearson $residuals$ are useful for interpreting the results of a chi-squared test when the $alternative$ hypothesis can be concluded to be the truth. They show a relative measurement of how much the $observed $ counts differ from the $expected$
counts.

## Q3
The following questions refer to the R Instructions and the HairEyeColor example of the Chi-squared Tests page.

 

Run the following code in R. (This data is fabricated to mimick reality, but is not actual data.)

```{r}
glasses <- cbind( Males = c(Glasses = 5, Contacts = 12, None = 18), Females = c(Glasses = 4, Contacts = 14, None = 22))

glasses

barplot(glasses, beside=TRUE, legend.text=TRUE, args.legend=list(x = "topleft", bty="n"))
```
 
### 1)

Which of the following statements are correct interpretations of the bar plot that the above code generated in R?

answer: 

The data shows that the most common result for either males or females is to not wear glasses or contacts. It also shows that the least common result for both genders is to wear glasses. In other words, the pattern for both genders is the same.

and 

The gender of an individual does not seem to be associated with whether a person wears glasses, contacts, or no eye correction because the pattern of the bars is essentially the same for males and females.

### 2)
The correct null and alternative hypotheses for these data are

$H_0$  Corrective eye wearing and gender are independent
$H_a$  Corrective eye wearing and gender are associated

### 3)
Run the following code in R to determine if a chi-squared test of independence is appropriate for the glasses data.

```{r}
chis.glasses <- chisq.test(glasses)

chis.glasses$expected
```
 

Are the requirements for this test satisfied?

answer: Yes, the requirements are met because the average of the expected counts is greater than 5 and all expected counts are greater than 1, even though some expected counts are less than 5.

### 4)
Run the following code in R to view the results of your test.

```{r}
chis.glasses
```

χ2=   0.3331

degrees of freedom =  2

p-value =  0.8466

### 5)
This confirms our original suspicion that we saw in the graphic. There is  $insufficient$ evidence to conclude that corrective eye wearing and gender are  $associated$. We will continue to assume the null hypothesis that whether someone wears glasses, contacts, or no corrective eye wear is $independent$ of their gender.

 

Since we failed to reject the null there is no interpretation to make for these data so we are not interested in the Pearson Residuals. However, so that you get the opportunity to see what the Pearson Residuals look like when we fail to reject, run the following code in R and notice that none of the residuals stand out as being exceptionally large in magnitude. This is always the case when we fail to reject the null hypothesis in a chi-squared test of independence.

> chis.glasses$residuals

## Q4

### 1)
These questions allow you to check your understanding of how to properly perform a Chi-squared Test of Independence in R.

 

Asia has become a major competitor with the U.S and Western Europe in education. The following table presents the counts of university degrees awarded to students in engineering and science (natural and social sciences) for the three regions.

 

 	 	Region	 
Field	United States	Western Europe	Asia
Engineering	61941	158931	280772
Natural Science	111158	140126	242879
Social Science	182166	116353	236018
 

The following code gets this data into R.

```{r}
education <- cbind( `United States` = c(Engineering = 61941, `Natural Science` = 111158, `Social Science` = 182166), `Western Europe` = c(Engineering = 158931, `Natural Science` = 140126, `Social Science` = 116353), Asia = c(280772, 242879, 236018))

education
```
 

Are there any differences in the numbers of degrees awarded to each field for the different regions? In other words, are field and region associated? Or can we assume that all three countries are similar in their patterns of degrees being awarded?


The null and alternative hypotheses for this study are:
$H_0$: Field and Region are independent
$H_a$: Field and Region are associated

```{r}
FandR.chi <- chisq.test(education)

FandR.chi$expected
FandR.chi$residuals

FandR.chi
```

### 2)
Create an appropriate graphic in R for this analysis. It should look like this:

<graphic>

```{r}
barplot(cbind( `United States` = c(Engineering = 61941, `Natural Science` = 111158, `Social Science` = 182166), `Western Europe` = c(Engineering = 158931, `Natural Science` = 140126, `Social Science` = 116353), Asia = c(280772, 242879, 236018)),
        beside = TRUE,
        main = "College Degrees Awarded by Region",
        legend.text = TRUE,
        args.legend =
          list(x = "topleft",
               bty = "n")
        )
```




### 3)
Obtain the Pearson Residuals for this analysis. To check that you obtained them correctly, enter the residual for Engineering in Western Europe:  


```{r}
FandR.chi$residuals
```
 

## Q5
This is a review question. It covers one of the topics we have previously studied this semester. It should help you review for the final exam.

Consider the InsectSprays dataset in R.

?InsectSprays

 
### 1)
Test the hypothesis that the mean number of bugs killed by insectisides A, B, C, D, E, and F are all the same against the alternative that at least one mean is different.

```{r}
insect <- table(InsectSprays$count, InsectSprays$spray)

Insect.chi <- chisq.test(insect) 

Insect.chi
Insect.chi$expected
Insect.chi$residuals

```


The p-value of the test is less than $.05$ showing that there is  $sufficient$ evidence to conclude that $\text{at least one bug spray results in a different}$ average number of bugs per agricultural experimental unit than the other sprays.

The appropriateness of the test is $questionable$ because the residual plot shows that the  $\text{constant (equal) variance}$ is $questionable$ and the Q-Q Plot suggests the errors are $\text{likely not normally distributed.}$


Thus, we should not make any conclusion about these insect sprays until next week when we can use the non-parametric version of ANOVA called the "Kruskal-Wallis Test." In other words, this test is inconclusive due to difficulties with the requirements not being satisfied.


# Assessment Quiz

## Q1

What type of statistical test would be appropriate for understanding how gender impacts the risk a person of a certain age has of having a heart attack?

Answer: Multiple Logistic Regression 

We are trying to predict an outcome, whether or not the person will experience a heart attack, using their age and gender. Age is a quantitative measurement, gender is a qualitative measurement, and the response variable Y could be 1 for a heart attack and 0 for no heart attack. This is the perfect scenario of a multiple logistic regression.

## Q2

Which statistical method would be best suited for determining how the age and gender of an individual can predict their annual income, measured in thousands of dollars?

Answer:  Multiple Linear Regression 

The response variable of this scenario, annual income measured in thousands, is quantitative. Since we have a quantitative explanatory variable of age and a qualitative explanatory variable of gender, a multiple linear regression would be the best choice.

## Q3

Run the following codes in R:


```{r}
plot((Wind > 10) ~ Temp, data=airquality, xlab="Daily Average Temperature", ylab="Probability Daily Average Wind Speed (mph) > 10", main="La Guardia Airport Measurements in 1973")

air.glm <- glm(Wind > 10 ~ Temp + as.factor(Month), data=airquality, family=binomial)

curve(exp(air.glm$coefficients[1]+air.glm$coefficients[2]*x)/(1 + exp(air.glm$coefficients[1] + air.glm$coefficients[2]*x)), add = TRUE)


```

During which month is there a significant change in the probability that the daily average wind speed will be greater than 10 mph, for all values of daily average temperature?




ANSWER: No change

```{r}
summary(glm(Wind > 10 ~ Temp + as.factor(Month), data=airquality, family=binomial))
# look at the p-values. They are all greater than .05 so fail to reject the null. 

```

